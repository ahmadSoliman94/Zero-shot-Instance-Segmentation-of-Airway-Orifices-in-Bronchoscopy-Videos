{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/mambaforge/envs/airway/lib/python3.11/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import find_contours\n",
    "from torch.cuda import amp\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import filters, morphology\n",
    "\n",
    "sys.path.append('./architectures/')\n",
    "from architectures.lr_aspp import LiteRASPP\n",
    "\n",
    "sys.path.append('./datasets/')\n",
    "from datasets import phantom_dataset\n",
    "from datasets.cvc_dataset import CVC_Dataset\n",
    "from datasets.real_dataset import RealSegDataset\n",
    "from datasets.broncho_dataset import Broncho_Dataset\n",
    "\n",
    "import utils\n",
    "import json\n",
    "\n",
    "# Import the new model and utilities\n",
    "sys.path.append('../')\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from util import ZSegmentationExtractor\n",
    "\n",
    "class DatasetEvaluator:\n",
    "    def __init__(self, dataset_type, img_resolution=128, gray_scaled=True):\n",
    "        \"\"\"\n",
    "        dataset_type: str - one of 'cvc', 'real', 'phantom', 'broncho'\n",
    "        \"\"\"\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.spatial_dim = img_resolution\n",
    "        self.gray_scaled = gray_scaled\n",
    "        self.model = self._setup_model()\n",
    "        self.dataset = self._setup_dataset(dataset_type)\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "    def _setup_model(self):\n",
    "        model_configs = {\n",
    "            'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "            'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "            'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "            'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "        }\n",
    "        encoder = 'vitl'\n",
    "        model = DepthAnythingV2(**model_configs[encoder])\n",
    "        model.load_state_dict(torch.load(f'../depth_anything_v2_{encoder}.pth', map_location=self.device, weights_only=True))\n",
    "        return model.to(self.device).eval()\n",
    "    \n",
    "    def _setup_dataset(self, dataset_type):\n",
    "        dataset_map = {\n",
    "            'cvc': lambda: CVC_Dataset(self.spatial_dim, normalize=False, gray_scale=self.gray_scaled),\n",
    "            'real': lambda: RealSegDataset(self.spatial_dim, normalize=False, gray_scale=self.gray_scaled),\n",
    "            'phantom': lambda: phantom_dataset.PhantomSegDataset('blob', 'test', self.spatial_dim, normalize=False, gray_scaled=self.gray_scaled),\n",
    "            'broncho': lambda: Broncho_Dataset(self.spatial_dim, normalize=False, gray_scale=self.gray_scaled)\n",
    "        }\n",
    "        \n",
    "        if dataset_type not in dataset_map:\n",
    "            raise ValueError(f\"Dataset type must be one of {list(dataset_map.keys())}\")\n",
    "            \n",
    "        return dataset_map[dataset_type]()\n",
    "\n",
    "    @staticmethod\n",
    "    def show_points(coords, labels, ax, marker_size=100):\n",
    "        cmap = plt.get_cmap('tab10')\n",
    "        unique_labels = np.unique(labels)\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            color = cmap(i % 10)\n",
    "            points = coords[labels == label]\n",
    "            ax.scatter(points[:, 1], points[:, 0], color=color, marker='o', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filtered(image, cutoffs, squared_butterworth=False, order=0.0, npad=32):\n",
    "        highpass_filtered = []\n",
    "        for cutoff in cutoffs:\n",
    "            highpass_filtered.append(\n",
    "                filters.butterworth(\n",
    "                    image,\n",
    "                    cutoff_frequency_ratio=cutoff,\n",
    "                    order=order,\n",
    "                    high_pass=True,\n",
    "                    squared_butterworth=squared_butterworth,\n",
    "                    npad=npad,\n",
    "                )\n",
    "            )\n",
    "        return highpass_filtered\n",
    "\n",
    "    def evaluate(self, sample_interval=1):\n",
    "        # Set up sampling for different datasets\n",
    "        if self.dataset_type == 'phantom':\n",
    "            indices = list(range(0, len(self.dataset), 30))\n",
    "        elif self.dataset_type == 'broncho':\n",
    "            indices = list(range(0, len(self.dataset), 5))\n",
    "        else:\n",
    "            indices = list(range(0, len(self.dataset), sample_interval))\n",
    "\n",
    "        num_diff = torch.empty(len(indices), dtype=torch.int32)\n",
    "        dice = torch.empty_like(num_diff, dtype=torch.float)\n",
    "        c_dist = torch.empty_like(dice)\n",
    "\n",
    "        # Set up specific parameters for each dataset\n",
    "        params = {\n",
    "            'cvc': {'corner_margin': 25, 'edge_margin': 7, 'intensity_threshold': 0.5},\n",
    "            'real': {'corner_margin': 25, 'edge_margin': 7, 'intensity_threshold': 0.5},\n",
    "            'phantom': {'corner_margin': 25, 'edge_margin': 7, 'intensity_threshold': 0.5},\n",
    "            'broncho': {'corner_margin': 25, 'edge_margin': 7, 'intensity_threshold': 0.5}\n",
    "        }\n",
    "\n",
    "        current_params = params[self.dataset_type]\n",
    "\n",
    "        for idx, i in enumerate(tqdm(indices)):\n",
    "            x, y = self.dataset[i]\n",
    "            with amp.autocast(), torch.no_grad():\n",
    "                # Preprocess the input image\n",
    "                raw_img = x.numpy()\n",
    "                resized_img = cv2.resize(raw_img.transpose(1, 2, 0), (128, 128))\n",
    "                \n",
    "                # Infer depth and perform segmentation\n",
    "                depth = self.model.infer_image(resized_img)\n",
    "                max_depth = depth.max()\n",
    "                inverted_depth = max_depth - depth\n",
    "                \n",
    "                # Apply high-pass filter to inverted depth\n",
    "                cutoffs = [0.0126, 0.05, 0.32]\n",
    "                highpass_depths = self.get_filtered(inverted_depth, cutoffs, squared_butterworth=False, order=5, npad=211)\n",
    "                high_pass_depth = highpass_depths[0]\n",
    "                \n",
    "                depth_tensor = torch.from_numpy(high_pass_depth)\n",
    "\n",
    "                # Initialize segmentation extractor with dataset-specific parameters\n",
    "                segmentation_extractor = ZSegmentationExtractor(\n",
    "                    self.spatial_dim, \n",
    "                    watershed_compactness=3, \n",
    "                    avg_pool_kernel_size=1, \n",
    "                    **current_params\n",
    "                )\n",
    "\n",
    "                # Extract predesegmentation\n",
    "                d = segmentation_extractor.extract_segmentation(depth_tensor, rgb_img=resized_img, return_plot_data=True)\n",
    "                seg_mask = d['seg_mask']\n",
    "                label_y_hat = d['z_labels']\n",
    "                c_y_hat = d['airway_centroids']\n",
    "                num_y_hat = len(d['z_labels'])\n",
    "\n",
    "                n_points = len(c_y_hat)\n",
    "                labels = np.arange(1, n_points + 1)\n",
    "\n",
    "                # Convert instance segmentation to binary\n",
    "                small_object_size = 100\n",
    "                \n",
    "                y_hat_cleaned = (seg_mask > 0).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "                y_hat_cleaned = morphology.remove_small_objects(y_hat_cleaned.astype(bool), min_size=small_object_size)\n",
    "                y_hat_cleaned = morphology.binary_opening(y_hat_cleaned, morphology.disk(3))\n",
    "                y_hat = y_hat_cleaned\n",
    "\n",
    "            y[y == -1] = 0\n",
    "            y = y.squeeze(0)\n",
    "\n",
    "            # Plot the results\n",
    "            self.plot_results(resized_img, high_pass_depth, y, y_hat, c_y_hat, labels, i)\n",
    "\n",
    "            assert torch.all(torch.isin(y.to(int), torch.tensor([0, 1])))\n",
    "\n",
    "            # Extract airways using ZSegmentationExtractor\n",
    "            label_y, c_y, num_y = utils.extract_airways(y.to(torch.bool).numpy())\n",
    "\n",
    "            # Calculate metrics\n",
    "            if num_y > 0:\n",
    "                dice[idx] = utils.dice_coeff(torch.from_numpy(y_hat), y, max_label=1)\n",
    "            else:\n",
    "                dice[idx] = -1\n",
    "\n",
    "            if num_y > 0 and num_y_hat > 0:\n",
    "                if isinstance(self.dataset, phantom_dataset.PhantomSegDataset) and False:  # in_mm is always False in original code\n",
    "                    # Handle phantom dataset specific calculations (keeping original logic)\n",
    "                    z = phantom_dataset.PhantomSegDataset('z', 'test', self.spatial_dim).gt\n",
    "                    z_y = torch.empty(num_y)\n",
    "                    contours = find_contours(y.numpy(), fully_connected='high')\n",
    "\n",
    "                    assert len(contours) == num_y\n",
    "                    for c_idx in range(len(contours)):\n",
    "                        z_contour = utils.sample_from_image(z[i], torch.from_numpy(contours[c_idx]).flip(-1))\n",
    "                        z_y[c_idx] = z_contour.median()\n",
    "\n",
    "                    c_y = phantom_dataset.pixel_to_normalized_world_coordinates(torch.as_tensor(c_y), [self.spatial_dim, self.spatial_dim])\n",
    "                    c_y_hat = phantom_dataset.pixel_to_normalized_world_coordinates(torch.as_tensor(c_y_hat), [self.spatial_dim, self.spatial_dim])\n",
    "\n",
    "                    d = c_y.unsqueeze(1) - c_y_hat.unsqueeze(0)\n",
    "                    d = d.norm(p=2, dim=-1).min(1)[0]\n",
    "                    c_dist[idx] = (d * z_y * phantom_dataset.Z_MAX_DEPTH).mean()\n",
    "                else:\n",
    "                    try:\n",
    "                        c_y_tensor = torch.as_tensor(c_y)\n",
    "                        c_y_hat_tensor = torch.as_tensor(c_y_hat)\n",
    "\n",
    "                        # Check for empty tensors\n",
    "                        if c_y_tensor.numel() == 0 or c_y_hat_tensor.numel() == 0:\n",
    "                            c_dist[idx] = -1\n",
    "                            continue\n",
    "\n",
    "                        # Ensure we have 2D coordinates\n",
    "                        if c_y_tensor.size(-1) != c_y_hat_tensor.size(-1):\n",
    "                            c_y_tensor = c_y_tensor[:, :2]\n",
    "                            c_y_hat_tensor = c_y_hat_tensor[:, :2]\n",
    "\n",
    "                        # Additional shape check\n",
    "                        if c_y_tensor.size(0) == 0 or c_y_hat_tensor.size(0) == 0:\n",
    "                            c_dist[idx] = -1\n",
    "                            continue\n",
    "\n",
    "                        # Calculate distances\n",
    "                        d = c_y_tensor.unsqueeze(1) - c_y_hat_tensor.unsqueeze(0)  # nxmx2\n",
    "                        if d.size(0) > 0 and d.size(1) > 0:\n",
    "                            d = d.norm(p=2, dim=-1)\n",
    "                            if d.size(1) > 0:\n",
    "                                d = d.min(1)[0]\n",
    "                                c_dist[idx] = d.mean()\n",
    "                            else:\n",
    "                                c_dist[idx] = -1\n",
    "                        else:\n",
    "                            c_dist[idx] = -1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Error calculating centroid distance for index {idx}: {e}\")\n",
    "                        c_dist[idx] = -1\n",
    "            else:\n",
    "                c_dist[idx] = -1\n",
    "\n",
    "            num_diff[idx] = num_y_hat - num_y\n",
    "\n",
    "        # Print metrics\n",
    "        self.print_metrics(dice, c_dist, num_diff)\n",
    "        \n",
    "        # Plot final statistics\n",
    "        self.plot_statistics(dice, c_dist, num_diff)\n",
    "\n",
    "    def plot_results(self, raw_img, depth_map, ground_truth, pred_mask, centroids, labels, index):\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(8, 8))\n",
    "\n",
    "        axes[0].imshow(raw_img)\n",
    "        axes[0].set_title(f'Raw Image {index}')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(depth_map, cmap='viridis')\n",
    "        axes[1].set_title(f'Inverted Depth {index}')\n",
    "        self.show_points(centroids, labels, plt.gca())\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        axes[2].imshow(ground_truth, cmap='gray')\n",
    "        axes[2].set_title(f'Ground Truth {index}')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        axes[3].imshow(pred_mask, cmap='gray')\n",
    "        axes[3].set_title(f'Predicted Mask {index}')\n",
    "        self.show_points(centroids, labels, plt.gca())\n",
    "        axes[3].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def print_metrics(self, dice, c_dist, num_diff):\n",
    "        # Filter out invalid measurements\n",
    "        dice = dice[dice != -1]\n",
    "        c_dist = c_dist[c_dist != -1]\n",
    "\n",
    "        print('Dataset:', self.dataset)\n",
    "        print(f'num diff centroids: {num_diff.float().mean().item()} +- {num_diff.float().std().item()}')\n",
    "        print(f'DSC: {dice.mean().item()} +- {dice.std().item()}, median: {dice.median().item()}')\n",
    "        print(f'D_c[px]: {c_dist.mean().item()} +- {c_dist.std().item()}, median: {c_dist.median().item()}')\n",
    "\n",
    "    def plot_statistics(self, dice, c_dist, num_diff):\n",
    "        plt.hist(num_diff.numpy(), bins='auto')\n",
    "        plt.xlabel('|C_predicted|-|C_ground truth|')\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.boxplot(dice.numpy(), showfliers=False, notch=True, showmeans=True)\n",
    "        plt.title('Dice mean')\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.boxplot(c_dist.numpy(), showfliers=False, notch=True, showmeans=True)\n",
    "        plt.title('Mean centroid distance')\n",
    "        plt.ylabel('distance [px]')\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVC DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = DatasetEvaluator('cvc')\n",
    "# evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hauser DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = DatasetEvaluator('real')\n",
    "# evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phantom DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = DatasetEvaluator('phantom')\n",
    "# evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BronchoLC DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = DatasetEvaluator('broncho')\n",
    "# evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Evaluating CVC Dataset\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:18<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: <datasets.cvc_dataset.CVC_Dataset object at 0x7f16fc5629d0>\n",
      "num diff centroids: 12801.8876953125 +- 0.6119245886802673\n",
      "DSC: 0.6239727139472961 +- 0.2181776463985443, median: 0.6757317185401917\n",
      "D_c[px]: 13.935718536376953 +- 7.137856483459473, median: 12.693500518798828\n",
      "\n",
      "Statistics Summary:\n",
      "Number of differences - Mean: 12801.89, Std: 0.61\n",
      "Dice Score - Mean: 0.62, Std: 0.22\n",
      "Centroid Distance - Mean: 13.94, Std: 7.14\n",
      "\n",
      "\n",
      "==================================================\n",
      "Evaluating REAL Dataset\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:17<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: <datasets.real_dataset.RealSegDataset object at 0x7f16fc560e10>\n",
      "num diff centroids: 12681.6357421875 +- 1237.546142578125\n",
      "DSC: 0.4214887320995331 +- 0.2711307108402252, median: 0.43695518374443054\n",
      "D_c[px]: 19.391002655029297 +- 14.375557899475098, median: 14.708398818969727\n",
      "\n",
      "Statistics Summary:\n",
      "Number of differences - Mean: 12681.64, Std: 1237.55\n",
      "Dice Score - Mean: 0.42, Std: 0.27\n",
      "Centroid Distance - Mean: 19.39, Std: 14.38\n",
      "\n",
      "\n",
      "==================================================\n",
      "Evaluating PHANTOM Dataset\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load data: 100%|██████████| 2/2 [00:00<00:00, 21.63it/s]\n",
      "100%|██████████| 173/173 [00:28<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: <datasets.phantom_dataset.PhantomSegDataset object at 0x7f170437a790>\n",
      "num diff centroids: 12526.3125 +- 1951.8133544921875\n",
      "DSC: 0.5992451906204224 +- 0.19701744616031647, median: 0.6424276828765869\n",
      "D_c[px]: 16.537973403930664 +- 10.855913162231445, median: 13.857728004455566\n",
      "\n",
      "Statistics Summary:\n",
      "Number of differences - Mean: 12526.31, Std: 1951.81\n",
      "Dice Score - Mean: 0.60, Std: 0.20\n",
      "Centroid Distance - Mean: 16.54, Std: 10.86\n",
      "\n",
      "\n",
      "==================================================\n",
      "Evaluating BRONCHO Dataset\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:37<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: <datasets.broncho_dataset.Broncho_Dataset object at 0x7f15f73437d0>\n",
      "num diff centroids: 12748.0791015625 +- 828.069580078125\n",
      "DSC: 0.5151475071907043 +- 0.17124058306217194, median: 0.5144803524017334\n",
      "D_c[px]: 19.54058074951172 +- 11.895088195800781, median: 16.930543899536133\n",
      "\n",
      "Statistics Summary:\n",
      "Number of differences - Mean: 12748.08, Std: 828.07\n",
      "Dice Score - Mean: 0.52, Std: 0.17\n",
      "Centroid Distance - Mean: 19.54, Std: 11.90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress all warnings\n",
    "\n",
    "# Temporarily modify both plotting methods to do nothing\n",
    "def no_plot(self, *args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def no_statistics_plot(self, dice, c_dist, num_diff):\n",
    "    # Just print the statistics without plotting\n",
    "    print(\"\\nStatistics Summary:\")\n",
    "    print(f\"Number of differences - Mean: {num_diff.float().mean().item():.2f}, Std: {num_diff.float().std().item():.2f}\")\n",
    "    print(f\"Dice Score - Mean: {dice[dice != -1].mean().item():.2f}, Std: {dice[dice != -1].std().item():.2f}\")\n",
    "    print(f\"Centroid Distance - Mean: {c_dist[c_dist != -1].mean().item():.2f}, Std: {c_dist[c_dist != -1].std().item():.2f}\")\n",
    "\n",
    "# Store the original methods\n",
    "original_plot_results = DatasetEvaluator.plot_results\n",
    "original_plot_statistics = DatasetEvaluator.plot_statistics\n",
    "\n",
    "# Replace the methods\n",
    "DatasetEvaluator.plot_results = no_plot\n",
    "DatasetEvaluator.plot_statistics = no_statistics_plot\n",
    "\n",
    "# Run evaluations\n",
    "datasets = ['cvc', 'real', 'phantom', 'broncho']\n",
    "\n",
    "for dataset_type in datasets:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating {dataset_type.upper()} Dataset\")\n",
    "    print('='*50)\n",
    "    \n",
    "    try:\n",
    "        evaluator = DatasetEvaluator(dataset_type)\n",
    "        evaluator.evaluate()\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {dataset_type} dataset: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    print()  # Add blank line between datasets\n",
    "\n",
    "# Restore the original methods\n",
    "DatasetEvaluator.plot_results = original_plot_results\n",
    "DatasetEvaluator.plot_statistics = original_plot_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
